1. On modifie le docker compose comme il le faut

0. Fichier ".env". Avant d'aller plus loin, il faut √©galement parler du fichier "".env". Il s'agit d'un fichier de configuration 
qui permet de d√©finir des variables d'environnement utilis√©es par le docker compose. Par d√©faut, Ton docker-compose.yml sait automatiquement 
chercher les variables d‚Äôenvironnement externes depuis un fichier .env plac√© dans le m√™me dossier que ton fichier docker-compose.yml,
 sans que tu aies besoin de le sp√©cifier manuellement. C‚Äôest un comportement standard de Docker Compose.
Les variables du .env sont ensuite disponibles via la syntaxe ${VAR_NAME} dans ton YAML.
La syntaxe ${VAR_NAME:-default_value} permet de d√©finir une valeur par d√©faut si la variable n'est pas d√©finie dans le fichier .env.
On va utiliser nos variables d'environnement de cette mani√®re. On va les utiliser pour :
- image de airflow
- ${_PIP_ADDITIONAL_REQUIREMENTS:-} (--> se renseigner la dessus afin de pouvoir ajouter des librairies pythons suppl√©mentaires)
- ${AIRFLOW_PROJ_DIR:-.} (--> r√©pertoire de travail pour les DAGs, scripts et donn√©es)
- $${HOSTNAME} --> mieux comprendre hostname
- ${AIRFLOW_UID}
- ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
- ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
- ${POSTGRES_USER}
- ${POSTGRES_PASSWORD}
- ${POSTGRES_DB}

0. Avant d'aller' plus loin, il est √©galement important d'expliquer ' comment fonctionne la gestion du volume dans docker.
A. DOCKER et syst√®me de fichiers
Un conteneur Docker est une bo√Æte isol√©e. Par d√©faut, il ne voit aucun fichier de ta machine locale. 
Tout ce que tu veux rendre visible doit √™tre explicitement partag√© via des volumes.
Les volumes sont mont√©s dans la section "volumes" de chauqe service qui doit acc√®der √† des fichiers (Airflow, Postgres, etc.). 
Exemple de montage :
services:
  airflow-webserver:
    image: apache/airflow:2.10.5
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./config:/opt/airflow/config
      - ./data:/opt/airflow/data
  
Il est √©galement possible de d√©finir des volumes nomm√©s (souvent utilis√©s pour des DB ou la persistance des logs). On le fait souvent √† la fin du docker-compose.
volumes:
  postgres-db-volume:
  airflow-logs:
Ensuite, on les attache √† un service en particulier :
services:
  postgres:
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
B. Types de fichiers d'un projet ETL
  1. Fichiers sources (en json, csv,..) stock√©s en local sur la machine. On voudra les lire dans notre DAG.
  2. Fichiers interm√©diaires (fichiers logs, csv, parquet,..) √©crit dans le conteneur ou via volumes. On voudra y acc√®der pour les debugs. (MIEUX COMPRENDRE)
  3. Fichiers de configuration (docker-compose.yaml, .env, DAGs, scripts python) : √©crit en local et mont√©s dans le conteneur. (MIEUX COMPRENDR)
  --> o√π stocke-t-on les scripts python et les fichiers de output.
C. Acc√®der √† un fichier local depuis un conteneur docker
On doit utliser un volume bind mount dans le docker-compose.yaml  :
 volumes:
  - ./dags:/opt/airflow/dags
  - ./logs:/opt/airflow/logs
  - ./plugins:/opt/airflow/plugins
  - ./scripts:/opt/airflow/scripts  # <--- tu peux rajouter ceci
  - ./data:/opt/airflow/data        # <--- si tu veux stocker ton JSON ici
Ce qui veut dire : dans le dossier depuis lequel on lance le docker-compose, le sous-dossiers ./dags (sur ma machine locale) 
est li√© au dossier /opt/airflow/dags (dans mon conteneur Docker). Donc si je veux acc√®der aux fichiers du dossier .dags, depuis docker, je dois appeler 
le sous dossier /opt/airflow/dags. 
Exemple : Ainsi, si ton JSON est dans ./data/monfichier.json sur ta machine locale, alors dans ton DAG tu y acc√®des via :
file_path = "/opt/airflow/data/monfichier.json"


D. Stocker des fichiers depuis Docker vers ma machine locale.
La logique est la m√™me, si j'√©cris un fichier dans un dossier mont√© en volume, alors ce fichier sera visible localement.
Par exemple : ./data : /opt/airflow/data
with open("/opt/airflow/data/output.json", "w") as f:
    json.dump(obj, f)
Alors, je pourrais r√©cuperer ce fichier cr√©er, dans mon sous dossier, ./data, en local.
Il est √©galement possivle de stock√© des fichiers de mani√®re temporaire, sur docker uniquement, si les fichiers ne sont pas stock√©s dans un dossier mont√© en volume.
Le fichier existe alors uniquement dans Docker et sera supprim√© √† la suppression du conteneur.
E. Bonnes pratiques :
1. Lire/√©crire des fichiers --> sous-dossier ./data mont√©
2. Scripts python --> sous dossier ./scripts mont√©
3. Fichiers temporaires --> /tmp de docker (gardez en t√™te que ca pourra √™tre supprim√©)
4. Stockage en DB --> utiliser un volume docker

F. Cas d'usage d'un service de DB (ex: Postgres, SQL Server, MySQL,...)
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data  # Persistance
volumes:
  postgres-db-volume:
Et voici les bonnes pratiques :
Pratique  Explication
‚úÖ Monter un volume persistant                        ----------->  Sinon les donn√©es sont perdues si tu arr√™tes/supprimes le conteneur
‚úÖ Utiliser des variables d‚Äôenvironnement             ---------->   Pour d√©finir les credentials sans les hardcoder
‚úÖ Se connecter via un service Docker (postgres)          ------>  Ex: host=postgres port=5432 au lieu de localhost
‚úÖ G√©rer les migrations ou initialisations avec des scripts ---->  Tu peux ajouter des scripts .sql ou .py ex√©cut√©s au d√©marrage




On peut continuer avec l'analyse' de notre docker compose.

A.  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.10.5}  : on utilise la version 2.10.5 plutot que la 3.0
B. on rajoute la ligne : AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30  # Fr√©quence (en secondes) √† laquelle le scheduler v√©rifie les nouveaux DAGs dans le dossier "dags"
C. on rajoute la ligne : AIRFLOW__CORE__TEST_CONNECTION: Enabled # Active le test automatique des connexions dans l‚Äôinterface UI (permet de valider facilement les connexions √† une base de donn√©es ou API)
D. on rajoute la ligne : AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'      # Permet d‚Äôafficher toute la configuration d‚ÄôAirflow dans l'interface web (menu "Admin" > "Configurations")
E. On d√©sactive les exemples : AIRFLOW__CORE__LOAD_EXAMPLES: 'false' # D√©sactive les DAGs d'exemple
F. On a rajout√© 2 lignes dans la partie volumes :
    - ${AIRFLOW_PROJ_DIR:-.}/scripts:/opt/airflow/scripts
    - ${AIRFLOW_PROJ_DIR:-.}/data:/opt/airflow/data
    Ces lignes permettent de lier les dossiers locaux "scripts", "data" (en plus de logs, dags,config et "plugins") au conteneur Airflow, ce qui permet de g√©rer les DAGs, les logs et les plugins depuis l'ext√©rieur du conteneur.
La question du volume dans Docker est importante car elle permet de garder les donn√©es persistantes m√™me si le conteneur est supprim√© ou recr√©√©. 
En liant des dossiers locaux, on s'assure que les modifications apport√©es aux DAGs, scripts ou donn√©es sont imm√©diatement refl√©t√©es 
dans le conteneur sans avoir √† reconstruire l'image.
Ce qu'on fait en pratique, c'est de cr√©er un lien entre un dossier local sur notre machine (ou notre serveur) et un dossier virtuel dans le conteneur Docker.
Dans notre cas, via airflow-common, on monte les volumes pour tout les services de airflow de notre docker compose. De cette mani√®re, tout les services de airflow
situ√©s dans diff√©rents conteneurs pourront acc√®der aux fichiers et dossiers, ce qui est essentiel pour le bon fonctionnement de notre ETL.

G. Il y a un service postgres, qui sera utilis√© pour stocker les m√©tadonn√©es d'Airflow. Dans notre cas, on ne modifie rien dans ce service.
H. IL y a aussi un service redis, qui sera utilis√© pour la gestion des files d'attente des t√¢ches. Dans notre cas, on ne modifie rien dans ce service.
I. Enfin, il y a les diff√©rents services de airflow : webserver, scheduler, worker,airflow-init, triggerer et flower. (mettre tableau qui explique).
Chacun de ces services a un r√¥le sp√©cifique dans l'architecture d'Airflow :
   - **webserver** : Interface utilisateur pour visualiser et g√©rer les DAGs.
   - **scheduler** : Planifie l'ex√©cution des t√¢ches dans les DAGs.
   - **worker** : Ex√©cute les t√¢ches planifi√©es par le scheduler.
   - **triggerer** : G√®re les d√©clencheurs d'√©v√©nements pour ex√©cuter des t√¢ches bas√©es sur des √©v√©nements externes.
   - **flower** : Outil de surveillance et de gestion des workers Airflow.
De plus, chacun de ces services est configur√© via un "airflow-common" qui est un conteneur partag√© entre tous les services de airflow.


J. Dans notre service airflow-init, on utilise nos variables d'environnement "username" et"password" pour cr√©er un utilisateur Airflow avec les droits d'administrateur.

K. On ajoute √©galement un autre service "postgreSQL", qui va nous servir comme base de donn√©es pour stocker les donn√©es de notre projet 
(attention, diff√©rent du service postgres qui est utilis√© pour stocker les m√©tadonn√©es d'Airflow!!!).
postgres_destination_server-project1 = nom du serveur
#On utilise les variables d'environnement :
POSTGRES_USER : ${POSTGRES_USER}
POSTGRES_PASSWORD : ${POSTGRES_PASSWORD}
POSTGRES_DB : ${POSTGRES_DB}
# ici on modifie le port de postgreSQL pour √©viter les conflits avec le service postgres d'Airflow
# (qui est sur le port 5432 par d√©faut) en le changeant en 5433.
Il y a 2 cas d'usages : on peut vouloir se connecter √† la base de donn√©es PostgreSQL depuis l'ext√©rieur du conteneur (par exemple, pour des requ√™tes SQL manuelles ou pour des applications externes)
ou on peut vouloir que d'autres services dans le m√™me r√©seau Docker acc√®dent √† cette base de donn√©es. Dans les deux cas, il est important de s'assurer que le port est correctement expos√© et mapp√©.
Si on veut s'y connecte depuis l'exterieur du conteneur, depuis notre machine locale, on devra utiliser host = localhost et port = 5433
Si on veut s'y connecter depuis un autre service de notre docker compose, on devra utiliser host = postgres_destination_server-project1 et port = 5432 (car dans le r√©seau Docker, le port est mapp√© √† 5432).
     ports: 
          - "5433:5432"

#ici on monte le volume pour la base de donn√©es PostgreSQL, afin de garder les donn√©es persistantes m√™me si le conteneur est supprim√© ou recr√©√©.
# La syntaxe est celle-ci :  <volume_source>:<container_destination>
volume_source = un volume docker nomm√© (d√©clar√© dans volumes:) ou un chemin local
container_destination = un chemin vers le conteneur Docker.
Dans notre cas, notre volume_source est un volume nomm√© Docker. Ce volume sera cr√©√© dans l'espace interne de docker, pas sur le disque directmeent accessible.
 Et les donn√©es PostgreSQL seront stock√©es dans ce volume, g√©r√© par Docker, pas dans un dossier local visible.

      volumes: 
          - pg-data-project1:/var/lib/postgresql/data/


L. On ajoute le dernier service dont on aura besoin : pg-admin, qui est une interface web graphique, utilis√©e pour g√©rer les bases de donn√©es PostgreSQL.
 On pourra donc acc√®der √† pgamdin via : localhost:8082
    ports:
      - "8082:80" 
On utilise √† nouveau des variables d environnement pour le username et le password de notre db.
--> Il faut ajouter la s√©curit√© de user et password par d√©faut au cas o√π les var d'environenemnt sont √† manquer.
On utilise √† nouveau un volume docker nomm√© pour persister les donn√©es de pgadmin (connexion, pr√©f√©rences,...)

M. Le dernier bloc d√©clare des volumes nomm√©s que Docker va g√©rer automatiquement
pour persister les donn√©es des services, m√™me si les conteneurs sont supprim√©s ou recr√©√©s.
  postgres-db-volume-project1:  #volume postgreSQL - Airflow metadata Database
  pg-data-project1: # volume PostgreSQL - Destination Database
  pgadmin-data-project1: # volume pgadmin - PostgreSQL interface




2. Une fois que le docker compose est bien d√©fini, on peut l'initialiser avec cette commande : docker compose -p airflow_project1 up airflow-init
Dans cette commande, on fait attention √©galement √† renommer le nom de notre projet de docker compose par "airflow_project1". En effet, comme je travialle sur plusieurs 
projets de airflow utilisant Docker, je veille √† garder une clart√© sur tout mes projets afin qu'ils ne se m√©langent pas.
--> comme je l'ai fait dans la d√©finition de notre docker compose.

3. Une fois que airflow est initialis√©, on peut lancer le projet avec la commande : docker compose -p airflow_project1 up
Une fois que tous les services se sont charg√©s, on peut acc√®der √† l'interface airflow via : localhost:8080
On s'y connecte avec les identifiants d√©finis dans '.env' :
_AIRFLOW_WWW_USER_USERNAME=admin_airflow
_AIRFLOW_WWW_USER_PASSWORD=admin_password

4. Ensuite, on se connecte sur l'interface pgamdin pour acc√®der √† notre serveur postgres (logiquement vide): localhost:8082
On s'y connecte avec les identifiants d√©finis dans '.env' :
PGADMIN_DEFAULT_EMAIL=admin_pgadmin@pgadmin.com
PGADMIN_DEFAULT_PASSWORD=admin_password

Nous allons donc configurer un nouveau serveur dans pgadmin pour acc√®der √† la DB de postgres de notre service.
--> question : est-il possible de cr√©er plusieurs DB au sein d'un m√™me service postgres? Utile?
On choisit un nom pour notre serveur du pgadmin : postgres_destination_server
Le host name= le nom de notre service dans docker : postgres_destination_serveur. Si pgadmin √©tait hors Docker, alors on choisirait localhost ou 127.0.0.1
Le port = 5432 (car c'est le port qui est expos√© dans notre docker compose pour ce service)
username = admin_postgres
password = admin_password
Une fois la configuration du serveur √©tabli, on peut voir que notre DB "worldwide_sport" existe bel et bien.

5. Nous allons cr√©er une "connection" dans Airflow pour nous connecter √† notre DB "worldwide_sport".
Cette connexion est la m√™me que celle que nous avons fait pour pgadmin, sauf qu'ici nous nommons une connection_id et non un serveur name.
connection_id = postgreSQL_connection (nom de la variable correspondant √† nnotre connection)
connectiontype = postgres       
host = postgres_destination_server      (nom du service dans docker)
login = admin_postgres       
password = admin_password
port = 5432
db = worldwide_sport


6. Maintenant que tout nos services et connexions sont bien configur√©es, on peut commencer √† cr√©er notre DAG.
# Import necessary libraries and modules

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.empty import EmptyOperator
from airflow.decorators import task
from airflow.utils.dates import days_ago
from datetime import datetime
from datetime import timedelta

# 0) On doit commencer par expliquer le docker-compose, l'utilisation des volumes, etc.. (pour ensuite r√©expliquer ici qu'on 
# reprend les volumes qui ont √©t√© mont√©s dans le docker-compose)

# A) We define the default arguments for the DAG.
default_args = {
    'owner': 'Augustin'                  # owner of the DAG : the person who created it
    ,'start_date': datetime(2025, 1, 1)  # define when the DAG should start running
    ,'email_on_failure': False            # to send an email on failure : we do not use it here
    ,'email_on_success': False            # to send an email on success : we do not use it here
    ,'email_on_retry': False              # to send an email on retry : we do not use it here
    ,'retries':2                         # number of retries in case of failure 
    ,'retry_delay': timedelta(minutes=3)  # delay between retries 
}

# B) We define the DAG itself.
ETL_dag = DAG(
    dag_id = 'ETL_analytics_sport_shop',                               # DAG ID
    default_args=default_args,                                         # Default arguments for the DAG (defined above)
    description='ETL process for the analytics of the sport shop',     # Description of the DAG
    tags=['ETL','PostgreSQL','Analytics','Docker','Flatfile','JSON'],  # Tags for the DAG
    catchup= False,                                                    # Do not backfill past runs when DAG is created 
                                                                       # Backfill means that Airflow will try to run the DAG for all past dates
    schedule='0 0 * * *'                                               # Schedule interval (run at midnight every day)
                                                                        # https://crontab.guru/ : very useful website to understand cron expressions
)



# C) We define the tasks in the DAG.

# 1) The start point of the DAG, represented by a DummyOperator
# expliquer √† quoi il sert
start = DummyOperator(
    task_id='start',
    dag=ETL_dag
)


# A good practice of Airflow is to separate the tasks into different scripts, and use a BashOperator to call these scripts.
# Indeed, it is easier to maintain and debug the code this way. 
# In our case, we will divide our ETL process into several scripts, each one responsible for a specific task:
# 1) Extract and transform the JSON file into a big table in PostgreSQL
# 2) Verify if the big table exists in PostgreSQL (this is just a check task) --> √† supprimer?
# 3) From the big table, we will create several tables in PostgreSQL : store_sales, daily_sales and payment_method. 
# We will have a separate script for each table. Each script will retrieve the data from the big table in postgreSQL,
# transform it and load it into the corresponding table in PostgreSQL.
extract_and_transform_JSON = BashOperator(
    task_id='extract_transform_big_table'
    ,bash_command='python /opt/airflow/scripts/extract_transform_big_table.py'
    #,execution_timeout = timedelta(minutes=1)
    ,dag=ETL_dag
)

# Ajout au DAG
check_table_task = BashOperator(
    task_id='check_big_table_exists'
    ,bash_command = 'python /opt/airflow/scripts/verify.py'
    ,dag=ETL_dag
)


transform_store_sales = BashOperator(
    task_id='transform_store_sales',
    bash_command='python /opt/airflow/scripts/transform_store_sales.py',
    dag=ETL_dag
)

transform_daily_sales = BashOperator(
    task_id='transform_daily_sales',
    bash_command='python /opt/airflow/scripts/transform_daily_sales.py',
    dag=ETL_dag
)

transform_payment_method = BashOperator(
    task_id='transform_payment_method',
    bash_command='python /opt/airflow/scripts/transform_payment_method.py',
    dag=ETL_dag
)

# 3) Once the tables are created, we will do some analysis and measurements on the data.
    # We will store these analysis into views in postgreSQL, to be used in further analysis or in reporting tools like PowerBI.
analysis_daily_sales = BashOperator(
    task_id='analysis_daily_sales',
    bash_command='python /opt/airflow/scripts/analysis_daily_sales.py',
    dag=ETL_dag
)

analysis_store_sales = BashOperator(
    task_id='analysis_store_sales',
    bash_command='python /opt/airflow/scripts/analysis_store_sales.py',
    dag=ETL_dag
)

analysis_payment_method = BashOperator(
    task_id='analysis_payment_method',
    bash_command='python /opt/airflow/scripts/analysis_payment_method.py',
    dag=ETL_dag
)

# 4) The end point of the DAG, represented by a DummyOperator
# expliquer √† quoi il sert 
stop = DummyOperator(
    task_id='stop',
    dag=ETL_dag
)


# D) We define the dependencies between the tasks in the DAG.
start >> extract_and_transform_JSON

extract_and_transform_JSON >> transform_store_sales
extract_and_transform_JSON >> transform_daily_sales
extract_and_transform_JSON >> transform_payment_method

transform_store_sales >> analysis_store_sales
transform_daily_sales >> analysis_daily_sales
transform_payment_method >> analysis_payment_method

[analysis_store_sales, analysis_daily_sales, analysis_payment_method] >> stop


Comme nous venons de le voir, notre DAG comporte 9 t√¢ches (dont 2 Dummy). Chacune des 7 t√¢ches poss√®de leur propre script python.
De plus, j'ai √©galement cr√©√© un module, appel√© "postgres_utils", qui comprend des fonctions que je vais r√©utiliser lors des mes diff√©rentes t√¢ches.
Il s'agit d'une fonction me permettant de me connecter √† postgresql ; une fonction stockant le r√©sultat d'une query dans un dataframe 
et une fonction permettant de cr√©er une table postgreSQL.
Etant donn√© qu'ici le focus n'est pas sur l'utilisation de postgresql en python, nous n'allons pas revenir sur l'explication de ces fonctions.
La seule chose sur laquelle je veux insister est la connexion √† postgresql. Il est possible de cr√©er une connection directe √† postgreSQL via 
la librairie d'airflow : PostgresHook.  
On peut se connecter directement √† notre base de donn√©es PostgreSQL en utilisant notre connection pr√©c√©demment cr√©√©e : postgreSQL_connection


from airflow.providers.postgres.hooks.postgres import PostgresHook
def connection_to_postgresql(postgres_conn_id):
    """
    Fonction qui se connecte √† une base de donn√©es PostgreSQL et renvoie un curseur.
    """
    hook = PostgresHook(postgres_conn_id=postgres_conn_id)
    conn = hook.get_conn()
    cur = conn.cursor()
    print("üìå Base utilis√©e :", conn.get_dsn_parameters().get('dbname'))
    return {'cur':cur,'conn':conn}


7. Une fois notre DAG cr√©√©, et les scripts √©galement cr√©√©s, nous pouvons attendre 30 secondes dans l'interface d'airflow pour que ce nouveau DAG apparaisse.
Ensuite, il devrait √™tre visible sous cette forme (ins√©rer photo).
On peut voir qu'il y a ' plusieurs parties interessantes :
 - il y a 10 taches (comme on l'a d√©finit')
 - il est possible de voir les t√¢ches sous forme de graphe. C'est une' autre mani√®re visuelle de voir les dependencies que l'on a d√©finies ' dans notre DAG. 
 - on peut lancer le DAG manuellement en haut √† droite.




